{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT - Lora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U accelerate==0.29.3 peft==0.10.0 bitsandbytes==0.43.1 transformers==4.40.1 trl==0.8.6 datasets==2.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-chroma\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index\n",
    "!pip install transformers einops accelerate langchain bitsandbytes\n",
    "!pip install sentence_transformers #Embedding\n",
    "!pip install llama_index\n",
    "!pip install llama-index-embeddings-langchain\n",
    "!pip install llama-index-llms-huggingface\n",
    "%pip install llama-index\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-embeddings-instructor\n",
    "# !pip install llama-index chromadb --quiet\n",
    "# !pip install chromadb\n",
    "# !pip install sentence-transformers\n",
    "# !pip install pydantic==1.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb\n",
    "%pip install llama-index-vector-stores-chroma\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 6/6 [00:27<00:00,  4.60s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext  #Vector store index is for indexing the vector\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    #system_prompt=system_prompt,\n",
    "    #tokenizer_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    #model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=\"beomi/Llama-3-Open-Ko-8B\",\n",
    "    device_map=\"auto\",\n",
    "    # loading model in 8bit for reducing memory\n",
    "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /abr/coss35/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Basic Model 한국어 모델\n",
    "# base_model = \"teddylee777/Llama-3-Open-Ko-8B-gguf\"  # 테디님의 Llama3 한국어 파인튜닝 모델\n",
    "base_model = \"beomi/Llama-3-Open-Ko-8B\"               # beomi님의 Llama3 한국어 파인튜닝 모델\n",
    "\n",
    "# 주가 증권 보고서 gemini 데이터셋\n",
    "hkcode_dataset = \"Dansoeun/Knu_fine_tun_dataset\"\n",
    "\n",
    "# 새로운 모델 이름\n",
    "new_model = \"Llama3-owen-Ko-3-8B-Dansoeun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<s>[INST]컴학 홍보단에 대해서 알려줘[/INST]https://cse.knu.ac.kr/sub6_2.php</s>'}\n"
     ]
    }
   ],
   "source": [
    "# llama 데이터 로드\n",
    "dataset = load_dataset(hkcode_dataset, split=\"train\")\n",
    "\n",
    "# 데이터 확인\n",
    "print( dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 6/6 [00:24<00:00,  4.02s/it]\n"
     ]
    }
   ],
   "source": [
    "### 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              base_model, \n",
    "              trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 파인튜닝\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2450' max='2450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2450/2450 35:00, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.718900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.384100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.822900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.530300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.372700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.345700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.264200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.180100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.157300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.136400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>0.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>0.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.146900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2450, training_loss=0.33640751468892, metrics={'train_runtime': 2106.0553, 'train_samples_per_second': 4.63, 'train_steps_per_second': 1.163, 'total_flos': 3.43219194556416e+16, 'train_loss': 0.33640751468892, 'epoch': 50.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_prompt_template():\n",
    "  return \"\"\"\n",
    "        당신은 경북대학교 정보 안내 챗봇으로, 질문에 대한 답을 해주어야합니다.\n",
    "        Casual Answer로 답변 형식을 지정합니다.\n",
    "        당신은 학습된 내용 안에서 질문에 답변해야합니다.\n",
    "        학습된 내용이 없을 경우에는, 연결된 DB내용을 참조하도록 합니다.\n",
    "        학습된 내용은 datasets항목 안에 있는 것을 뜻하며,\n",
    "        DB 연결은 아직 하지 않아, DB를 참조할 경우 \"내용을 찾지 못했습니다.\"라고 답하여야합니다.\n",
    "        당신은 스스로 답변 가치를 생성할 수 있습니다.\n",
    "        null이나 빈 값은 반환하지 않습니다.\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] 우리학교 상징 동물이 뭐야? [/INST]경북대학교 상징동물은 ‘호반우’로 칡소입니다. 호랑이와 같이 검은색 줄무늬가 있는 한국 고유의 전통소로 역사성, 봉사성, 상징성 모두를 갖추고 있습니다. 호반우는 ‘통합의 상징’으로서 남북교류 시기에 사용되기도 합니다.</li></ol> <p>2. 호랑이와 관련된 상징은 무엇이 있나요?[/INST] ‘호’는 호랑이와 관련된 모든 상징을 의미합니다. 호랑이와 관련된 상징으로는 ‘호랑이사당’이 있습니다. 사당은 제사를 지내는 곳으로, 호랑이사당에서는 연간 2회(3월, 9월) 제사가 지납니다. 호랑\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"우리학교 상징 동물이 뭐야?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련이 완료된 모델을 'new_model'에 저장 \n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 6/6 [00:21<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# 모델 튜닝을 위한 라이브러리\n",
    "from peft import LoraConfig, PeftModel  \n",
    "\n",
    "# base_model과 new_model에 저장된 LoRA 가중치를 통합하여 새로운 모델을 생성\n",
    "model_name=\"beomi/Llama-3-Open-Ko-8B\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model) # LoRA 가중치를 가져와 기본 모델에 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name=\"beomi/Llama-3-Open-Ko-8B\"\n",
    "\n",
    "# 사전 훈련된 토크나이저를 다시 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  \n",
    "\n",
    "# 토크나이저의 패딩 토큰을 종료 토큰(end-of-sentence token)과 동일하게 설정\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "# 패딩을 시퀀스의 오른쪽에 적용\n",
    "tokenizer.padding_side = \"right\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma DB connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, ServiceContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 4/4 [01:33<00:00, 23.46s/it]\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 4/4 [00:22<00:00,  5.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "base=\"Dansoeun/Llama3-owen-Ko-3-8B-Dansoeun\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Dansoeun/Llama3-owen-Ko-3-8B-Dansoeun\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Dansoeun/Llama3-owen-Ko-3-8B-Dansoeun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:  33%|███████████████████▎                                      | 2/6 [00:21<00:44, 11.17s/it]"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMetadata:\n",
    "    def __init__(self, context_window, num_output):\n",
    "        self.context_window = context_window\n",
    "        self.num_output = num_output\n",
    "\n",
    "# 메타데이터 객체 생성\n",
    "metadata = ModelMetadata(context_window=1024, num_output=512)\n",
    "\n",
    "# 모델에 메타데이터 속성 추가\n",
    "model.metadata = metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "Settings.llm=model\n",
    "Settings.embed_model=embed_model\n",
    "Settings.tokenzier=tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "#chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"txt_db\").load_data()\n",
    "\n",
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n",
    "# Query Data\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"경북대 공지사항 사이트 알려줘\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-retrievers-bm25\n",
    "!pip install KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.vector_stores import MetadataFilter, MetadataFilters, FilterOperator\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup prompts - specific to StableLM\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup prompts - specific to StableLM\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|>#         \n",
    "        당신은 경북대학교 정보 안내 챗봇으로, 질문에 대한 답을 해주어야합니다.\n",
    "        Casual Answer로 답변 형식을 지정합니다.\n",
    "        당신은 학습된 내용 안에서 질문에 답변해야합니다.\n",
    "        학습된 내용이 없을 경우에는, 연결된 DB내용을 참조하도록 합니다.\n",
    "        학습된 내용은 datasets항목 안에 있는 것을 뜻하며,\n",
    "        DB 연결은 아직 하지 않아, DB를 참조할 경우 \"내용을 찾지 못했습니다.\"라고 답하여야합니다.\n",
    "        당신은 스스로 답변 가치를 생성할 수 있습니다.\n",
    "        null이나 빈 값은 반환하지 않습니다.\n",
    "        url 반환 시, <SYSTEM>을 붙이지 않습니다.\n",
    "\n",
    "        Question: 크누큐브 사이트 알려줘\n",
    "        Answer: https://knucube.knu.ac.kr/\n",
    "\n",
    "        Question: 국어국문학과 전공 커리큘럼 사이트를 알려줘\n",
    "        Answer: https://home.knu.ac.kr/HOME/knujob/sub.htm?nav_code=knu1668399325&code=dept1&dept=dept-korean\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/coss35/anaconda3/envs/llm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 4/4 [01:56<00:00, 29.00s/it]\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "model= HuggingFaceLLM(\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.25, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"Dansoeun/Llama3-owen-Ko-3-8B-Dansoeun\",\n",
    "    model_name=\"Dansoeun/Llama3-owen-Ko-3-8B-Dansoeun\",\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    # model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm=model\n",
    "Settings.embed_model=embed_model\n",
    "Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"txt_db\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# initialize node parser\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "import Stemmer\n",
    "\n",
    "# We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=2,\n",
    "    # Optional: We can pass in the stemmer and set the language for stopwords\n",
    "    # This is important for removing stopwords and stemming the query + text\n",
    "    # The default is english for both\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from konlpy.tag import Okt  # konlpy의 Okt 형태소 분석기 import\n",
    "\n",
    "# 한국어 불용어 리스트\n",
    "korean_stopwords = ['이', '그', '저', '있다', '하다', '것', '들', '때', '등', '에서']\n",
    "\n",
    "# konlpy 기반의 Okt 토크나이저를 활용하여 텍스트를 한국어로 처리하는 BM25 리트리버 설정\n",
    "class KonlpyTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Okt()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # konlpy의 morphs 함수를 사용하여 형태소 단위로 토큰화\n",
    "        tokens = self.tokenizer.morphs(text)\n",
    "        # 불용어 제거\n",
    "        tokens = [token for token in tokens if token not in korean_stopwords]\n",
    "        return tokens\n",
    "\n",
    "# Korean BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=2,\n",
    "    stemmer=None,  # PyStemmer 대신 직접 토크나이저에서 처리\n",
    "    language=None,  # 언어 기본 처리를 비활성화\n",
    "    tokenizer=KonlpyTokenizer(),  # konlpy 기반의 토크나이저\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding newlines for mmindex: 100%|███████████████████████████████████████████████████| 410k/410k [00:00<00:00, 177MB/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_retriever.persist(\"./bm25_retriever\")\n",
    "\n",
    "loaded_bm25_retriever = BM25Retriever.from_persist_dir(\"./bm25_retriever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"dense_vectors\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=docstore, vector_store=vector_store\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [\n",
    "        index.as_retriever(similarity_top_k=2),\n",
    "        BM25Retriever.from_defaults(\n",
    "            docstore=index.docstore, similarity_top_k=2\n",
    "        ),\n",
    "    ],\n",
    "    num_queries=1,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 99b1b33c-97cf-402a-a7f7-6908e25fbd3c<br>**Similarity:** 5.2211761474609375<br>**Text:** 스마트러닝센터의 원격 콘텐츠 수업 개발과정은 다음과 같습니다 : 원격수업 콘텐츠 개발 신청 접수 및 선정평가 → 오리엔테이션 실시 → 교안 저작권 검토 → 영상 촬영 및 편집 → 최종 영상 출력물 검토 및 안내 → LCMS 탑재 안내 및 지원\r\n",
       "\r\n",
       "스마트러닝센터에서는 교육용 콘텐츠 영상 제작 또한 지원하고 있습니다.\r\n",
       "\r\n",
       "스마트러닝센터의 제작 지원을 신청할 수 있는 대상은 경북대학교 소속 전임교원 및 기관입니다.\r\n",
       "\r\n",
       "스마트러닝센터의 스튜디오는 3D, 호리즌, 블랙 스튜디오 등의 최첨단 콘텐츠 제작실입니다.\r\n",
       "\r\n",
       "=============================\r\n",
       "\r\n",
       "https://ufis.knu.ac.kr/ufis/index.do\r\n",
       "\r\n",
       "**경북대학교 대학시설공간 종합관리시스템**\r\n",
       "\r\n",
       "대학시설공간 종합관리시스템 사이트는 공간현황관리 및 공간조정관리, 공간비용관리, 시설정보관리, 사업수행관리, 전자도면 관리, 시스템관리의 7가지 갈래로 나뉩니다.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** a9612121-ae28-458d-ba83-b0dbf50f8325<br>**Similarity:** 5.2211761474609375<br>**Text:** =============================\r\n",
       "\r\n",
       "https://ufis.knu.ac.kr/ufis/index.do\r\n",
       "\r\n",
       "**경북대학교 대학시설공간 종합관리시스템**\r\n",
       "\r\n",
       "대학시설공간 종합관리시스템 사이트는 공간현황관리 및 공간조정관리, 공간비용관리, 시설정보관리, 사업수행관리, 전자도면 관리, 시스템관리의 7가지 갈래로 나뉩니다.\r\n",
       "\r\n",
       "=============================\r\n",
       "\r\n",
       "https://safe.knu.ac.kr/\r\n",
       "\r\n",
       "**경북대학교 연구실안전관리센터**\r\n",
       "\r\n",
       "연구실안전관리센터는 세계 초일류 연구중심대학이 되기 위해 연구활동을 수행하는 교수, 연구원, 대학(원)생 등의 연구활동종사자가 안전하고 쾌적한 환경에서 연구할 수 있도록 하기 위해 설립되었습니다.\r\n",
       "\r\n",
       "연구실안전관리센터는 연구실에서 발생하는 각종 안전사고에 대하여 대비·대응·예방 등 적극 지원을 통해 무재해·무사고를 도모하고자합니다.\r\n",
       "\r\n",
       "연구실안전관리센터에서는 연구실 안전에 관한 기본 걔획을 수립하고, 안전과 관련된 정기점검, 특별안전점검 및 정밀안전진단 관련 업무를 수행합니다.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieved_nodes = bm25_retriever.retrieve(\"경북대학교 대학시설공간 종합관리시스템\")\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://knucube.knu.ac.kr/HOME/global/sub.htm?nav_code=glo1549935238</ASSISTANT|>\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"비교과 교육과정 통합관리 시스템  알려줘\")\n",
    "#print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 4/4 [00:22<00:00,  5.53s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:llama_index.retrievers.bm25.base:The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "DEBUG:bm25s:Building index from IDs objects\n",
      "Finding newlines for mmindex: 100%|███████████████████████████████████████████████████| 410k/410k [00:00<00:00, 296MB/s]\n",
      "DEBUG:bm25s:Building index from IDs objects\n",
      "/raid/coss35/anaconda3/envs/llm/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.25` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cse.knu.ac.kr/sub03/sub03_01.html</ASSISTANT|>\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import PromptTemplate\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from konlpy.tag import Okt  # konlpy의 Okt 형태소 분석기 import\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "import chromadb\n",
    "import nest_asyncio\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "# Define a directory to persist index data\n",
    "PERSIST_DIR = \"./storage\"\n",
    "\n",
    "# 한국어 불용어 리스트\n",
    "korean_stopwords = ['이', '그', '저', '있다', '하다', '것', '들', '때', '등', '에서']\n",
    "\n",
    "        # konlpy 기반의 Okt 토크나이저를 활용하여 텍스트를 한국어로 처리하는 BM25 리트리버 설정\n",
    "class KonlpyTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Okt()\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        # konlpy의 morphs 함수를 사용하여 형태소 단위로 토큰화\n",
    "        tokens = self.tokenizer.morphs(text)\n",
    "        # 불용어 제거\n",
    "        tokens = [token for token in tokens if token not in korean_stopwords]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class RAGService:\n",
    "    def __init__(self):\n",
    "        ###MODEL 구축##################\n",
    "        system_prompt=\"\"\"\n",
    "        <|SYSTEM|>#         \n",
    "        당신은 경북대학교 정보 안내 챗봇으로, 질문에 대한 답을 해주어야합니다.\n",
    "        Casual Answer로 답변 형식을 지정합니다.\n",
    "        당신은 학습된 내용 안에서 질문에 답변해야합니다.\n",
    "        학습된 내용이 없을 경우에는, 연결된 DB내용을 참조하도록 합니다.\n",
    "        학습된 내용은 datasets항목 안에 있는 것을 뜻하며,\n",
    "        DB 연결은 아직 하지 않아, DB를 참조할 경우 \"내용을 찾지 못했습니다.\"라고 답하여야합니다.\n",
    "        당신은 스스로 답변 가치를 생성할 수 있습니다.\n",
    "        null이나 빈 값은 반환하지 않습니다.\n",
    "        url 반환 시, <SYSTEM>을 붙이지 않습니다.\n",
    "\n",
    "        Question: 크누큐브 사이트 알려줘\n",
    "        Answer: https://knucube.knu.ac.kr/\n",
    "\n",
    "        Question: 국어국문학과 전공 커리큘럼 사이트를 알려줘\n",
    "        Answer: https://home.knu.ac.kr/HOME/knujob/sub.htm?nav_code=knu1668399325&code=dept1&dept=dept-korean\n",
    "        \"\"\"\n",
    "        # This will wrap the default prompts that are internal to llama-index\n",
    "        query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "        model= HuggingFaceLLM(\n",
    "            context_window=2048,\n",
    "            max_new_tokens=256,\n",
    "            generate_kwargs={\"temperature\": 0.25, \"do_sample\": False},\n",
    "            system_prompt=system_prompt,\n",
    "            query_wrapper_prompt=query_wrapper_prompt,\n",
    "            tokenizer_name=\"Dansoeun/Llama3-owen-Ko-3-8B-Dansoeun\",\n",
    "            model_name=\"Dansoeun/Llama3-owen-Ko-3-8B-Dansoeun\",\n",
    "            device_map=\"auto\",\n",
    "            tokenizer_kwargs={\"max_length\": 2048},\n",
    "            # uncomment this if using CUDA to reduce memory usage\n",
    "            # model_kwargs={\"torch_dtype\": torch.float16}\n",
    "        )\n",
    "        embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "        \n",
    "        Settings.llm=model\n",
    "        Settings.embed_model=embed_model\n",
    "        Settings.chunk_size = 512\n",
    "        ########RAG##########################\n",
    "        # load documents\n",
    "        documents = SimpleDirectoryReader(\"txt_db\").load_data()\n",
    "        \n",
    "        # initialize node parser\n",
    "        splitter = SentenceSplitter(chunk_size=512)\n",
    "        \n",
    "        nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "        konlpytokenizer=KonlpyTokenizer()\n",
    "        \n",
    "        # Korean BM25 retriever\n",
    "        bm25_retriever = BM25Retriever.from_defaults(\n",
    "            nodes=nodes,\n",
    "            similarity_top_k=2,\n",
    "            stemmer=None,  # PyStemmer 대신 직접 토크나이저에서 처리\n",
    "            language=None,  # 언어 기본 처리를 비활성화\n",
    "            tokenizer=KonlpyTokenizer(),  # konlpy 기반의 토크나이저\n",
    "        )\n",
    "        \n",
    "        bm25_retriever.persist(\"./bm25_retriever\")\n",
    "        loaded_bm25_retriever = BM25Retriever.from_persist_dir(\"./bm25_retriever\")\n",
    "\n",
    "        docstore = SimpleDocumentStore()\n",
    "        docstore.add_documents(nodes)\n",
    "        \n",
    "        db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        chroma_collection = db.get_or_create_collection(\"dense_vectors\")\n",
    "        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "        \n",
    "        storage_context = StorageContext.from_defaults(\n",
    "            docstore=docstore, vector_store=vector_store\n",
    "        )\n",
    "        \n",
    "        #index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)\n",
    "        self.index=VectorStoreIndex(nodes=nodes, storage_context=storage_context)\n",
    "        #nest_asyncio.apply()\n",
    "\n",
    "        self.retriever = QueryFusionRetriever(\n",
    "            [\n",
    "                self.index.as_retriever(similarity_top_k=2),\n",
    "                BM25Retriever.from_defaults(\n",
    "                    docstore=self.index.docstore, similarity_top_k=2\n",
    "                ),\n",
    "            ],\n",
    "            num_queries=1,\n",
    "            use_async=True,\n",
    "        )\n",
    "    def query(self, query: str) -> str:\n",
    "        query_engine = RetrieverQueryEngine(self.retriever)  \n",
    "        response = query_engine.query(query)\n",
    "        return str(response)\n",
    "\n",
    "        \n",
    "\n",
    "rag_service=RAGService()\n",
    "       \n",
    "query_result=rag_service.query(\"경북대 컴학 알려줘 \")\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DB 구성 깔끔하게 해야할듯.. ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Query Engine with Custom Retrievers - Simple Hybrid Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
